{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter value of gamma\n",
      "0.9\n",
      "Value Iteration result\n",
      "V optimal-\n",
      "[[0.70078329 0.24866905 2.82187146]\n",
      " [0.61830574 0.52234926 0.59964066]\n",
      " [0.63856342 0.73155129 0.45047224]]\n",
      "\n",
      "Optimal policy\n",
      "[['d' '>' 'd']\n",
      " ['^' 'd' '^']\n",
      " ['>' '<' '<']]\n",
      "Total updates = 329\n",
      "\n",
      "Policy Iteration results-\n",
      "\n",
      "V optimal\n",
      "[[0.70078329 0.24866905 2.82187146]\n",
      " [0.61830574 0.52234926 0.59964066]\n",
      " [0.63856342 0.73155129 0.45047224]]\n",
      "Optimal policy\n",
      "[['d' '>' 'd']\n",
      " ['^' 'd' '^']\n",
      " ['>' '<' '<']]\n",
      "None\n",
      "Total updates = 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "class world:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\n",
    "\t\t# Reward on current state\n",
    "\t\tself.rmatrix = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "\t\t# For all states on all 4 actions probability to reach other state\n",
    "\t\t# Actions in order: W, E, N, S\n",
    "\n",
    "\t\t# 3D array: S X A X S\n",
    "\t\tself.M = np.zeros([9, 4, 9])\n",
    "\n",
    "\t\tself.gamma = 0\n",
    "\n",
    "\n",
    "\t#randomly fill values in M\n",
    "\tdef populateM(self, S, A):\n",
    "\n",
    "\t\tfor i in range(0, S):\n",
    "\n",
    "\t\t\tseed = np.random.rand(1)\n",
    "\t\t\tp = (1 - seed)/(S-1)\n",
    "\t\n",
    "\t\t\tfor a in range(0, A):\n",
    "\n",
    "\t\t\t\tfor j in range(0, S):\n",
    "\n",
    "\t\t\t\t\tif i == j:\n",
    "\t\t\t\t\t    self.M[i, a, j] = seed\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t    self.M[i, a, j] = p \n",
    "\n",
    "\t\n",
    "\tdef U(self, Vin):\n",
    "\n",
    "\t\t#Vin is S X 1 matrix\n",
    "\n",
    "\t\tS = len(Vin)\n",
    "\t\tVorg = np.copy(Vin)\n",
    "\t\tVnew = np.copy(Vin)\n",
    "\n",
    "\t\tfor i in range(0, S):\t# for all rows Vin, for all states\n",
    "\t\t\ttemp = []\t\t\t\n",
    "\n",
    "\t\t\tfor a in range(0, 4):\t\t# check max in all actions\n",
    "\n",
    "\t\t\t\tp = 0\n",
    "\t\t\t\tfor j in range(0, S):\n",
    "\t\t\t\t\tp = p + self.M[i, a, j]*Vorg[j] \n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\tt = self.rmatrix[i] + (self.gamma)*(p)\n",
    "\t\t\t\ttemp.append(t)\n",
    "\n",
    "\t\t\t#print temp\n",
    "\t\t\tVnew[i] = max(temp)\n",
    "\n",
    "\t\treturn Vnew\n",
    "\n",
    "\tdef policy_evaluation(self, P, V):\n",
    "\n",
    "\t\tS = len(P)\n",
    "\t\tVnew = np.copy(V)\n",
    "\n",
    "\n",
    "\t\twhile 1:\n",
    "\t\t\told_V = Vnew\n",
    "\t\t\t#V_init = w.U(V_init)\n",
    "\n",
    "\t\t\tfor i in range(0, S):\t\t\t\n",
    "\t\t\t\tsum = 0\n",
    "\t\t\t\tfor j in range(0, S):\n",
    "\t\t\t\t\tsum = sum + self.M[i, int(P[i]), j]*V[j]\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\tVnew[i] = self.rmatrix[i] + (self.gamma)*(sum)\n",
    "\n",
    "\n",
    "\t\t\tif np.array_equal(old_V, Vnew) :\n",
    "\t\t\t    break\n",
    "\t\t\n",
    "\t\treturn Vnew\n",
    "\n",
    "#Convert 1D V to 2D V\n",
    "def getV(V_init):\n",
    "\n",
    "\tV = np.zeros([3,3])\n",
    "\tind = 0\n",
    "\n",
    "\tfor i in range(0, 3):\n",
    "\t\tfor j in range(0, 3):\n",
    "\t\t\tV[i, j] = float(V_init[ind])\n",
    "\t\t\tind += 1\n",
    "\n",
    "\treturn V\n",
    "\n",
    "#Convert 2D P to 1D\n",
    "def getP(P):\n",
    "\n",
    "\tN = len(P[0])\n",
    "\tPnew = np.empty([N*N])\n",
    "\tind = 0\n",
    "\n",
    "\tfor i in range(0, N):\n",
    "\t\tfor j in range(0, N):\n",
    "\t\t\tPnew[ind] = P[i, j]\n",
    "\t\t\tind += 1\n",
    "\n",
    "\treturn Pnew\n",
    "\n",
    "#Get policy from value function\n",
    "def getPolicy(V):\n",
    "\n",
    "\tN = len(V[0])\n",
    "\tPolicy = np.empty([N,N],dtype=int)\n",
    "\n",
    "\tfor i in range(0, N):\n",
    "\t\tfor j in range(0, N):\n",
    "\t\t\tmax = 0\n",
    "\n",
    "\t\t\t# Note Im not comparing i,j value\n",
    "\t\t\t# ie no stay option\n",
    "\n",
    "\t\t\t# look at all four neighbours\n",
    "\t\t\tif i-1 >=0:\t\t\t\t\t\t#Up\n",
    "\t\t\t\tif V[i-1, j] > max:\n",
    "\t\t\t\t\tmax = V[i-1, j]\n",
    "\t\t\t\t\tPolicy[i, j] = 0\n",
    "\n",
    "\t\t\tif i+1 < N:\t\t\t\t\t\t# Down\n",
    "\t\t\t\tif V[i+1, j] > max:\n",
    "\t\t\t\t\tmax = V[i+1, j]\n",
    "\t\t\t\t\tPolicy[i, j] = 1\n",
    "\t\t\t\n",
    "\t\t\tif j+1 < N:\t\t\t\t\t\t# Right\n",
    "\t\t\t\tif V[i, j+1] > max:\n",
    "\t\t\t\t\tmax = V[i, j+1]\n",
    "\t\t\t\t\tPolicy[i, j] = 2\n",
    "\n",
    "\t\t\tif j-1 >=0:\t\t\t\t\t\t# Left\n",
    "\t\t\t\tif V[i, j-1] > max:\n",
    "\t\t\t\t\tmax = V[i, j-1]\n",
    "\t\t\t\t\tPolicy[i, j] = 3\n",
    "\n",
    "\treturn Policy\n",
    "\n",
    "#For printing directions\n",
    "def printPolicy(P):\n",
    "\n",
    "\tN = len(P[0])\n",
    "\tD = np.empty([N, N], dtype=str)\n",
    "\n",
    "\tfor i in range(0, N):\n",
    "\t\tfor j in range(0, N):\n",
    "\n",
    "\t\t\tif P[i, j] == 0:\t\t#Up\n",
    "\t\t\t\tD[i, j] = '^'\n",
    "\t\t\tif P[i, j] == 1:\t\t#Down\n",
    "\t\t\t\tD[i, j] = 'd'\n",
    "\t\t\tif P[i, j] == 2:\t\t#Right\n",
    "\t\t\t\tD[i, j] = '>'\n",
    "\t\t\tif P[i, j] == 3:\t\t#Left\n",
    "\t\t\t\tD[i, j] = '<'\n",
    "\n",
    "\tprint(D)\n",
    "\t\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "\tprint('Enter value of gamma')\n",
    "\tgamma = float(input())\n",
    "\t\n",
    "\tw = world()\n",
    "\tw.gamma = gamma\n",
    "\n",
    "\t# stay in state value\n",
    "\tS = 9\n",
    "\tA = 4\n",
    "\n",
    "\tV_init = np.zeros([S])\n",
    "\tw.populateM(S, A)\n",
    "\n",
    "\tupdates1 = 0\n",
    "\n",
    "\twhile 1:\n",
    "\t\told_V_int = V_init\n",
    "\t\tV_init = w.U(V_init)\n",
    "\t\tupdates1 += 1\n",
    "\n",
    "\t\tif np.array_equal(old_V_int, V_init) :\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\tV_opt = getV(V_init)\n",
    "\tprint('Value Iteration result')\n",
    "\tprint ('V optimal-')\n",
    "\tprint (V_opt)\n",
    "\tprint ('\\nOptimal policy')\n",
    "\tprintPolicy(getPolicy(V_opt))\n",
    "\tprint ('Total updates =',updates1)\n",
    "\n",
    "\tprint ('\\nPolicy Iteration results-')\n",
    "\n",
    "\t#Arbitrary policy\n",
    "\tpolicy = np.zeros([S])\n",
    "\t#Arbitrary V\n",
    "\tVnew = V_init\n",
    "\n",
    "\tupdates2 = 0\n",
    "\n",
    "\twhile 1:\n",
    "\n",
    "\t\tpolicy_old = policy\n",
    "\n",
    "\t\t#Policy evaluation\n",
    "\t\tVnew = w.policy_evaluation(policy, Vnew)\n",
    "\n",
    "\t\t#Policy Improvement\n",
    "\t\tptemp = getPolicy(getV(Vnew))\t#returns 2D\n",
    "\n",
    "\t\tpolicy = getP(ptemp)\t\t\t#save as 1D\n",
    "\n",
    "\t\tupdates2 += 1\n",
    "\t\tif np.array_equal(policy, policy_old):\n",
    "\t\t\tbreak\t\n",
    "\n",
    "\tprint('\\nV optimal')\n",
    "\tprint (getV(Vnew))\n",
    "\tprint ('Optimal policy')\n",
    "\tprint (printPolicy(ptemp))\n",
    "\tprint ('Total updates =',updates2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
